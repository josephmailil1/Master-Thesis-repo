{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josephmailil1/Speciale---Stibo/blob/main/AGENT%20RAG\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DKBssQelOeBe",
        "outputId": "0f755e43-bf9a-495f-86f8-78b440bc3eb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.3/299.3 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.0/116.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain langchain-openai langchain-community langchain-experimental pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OpenAI_Key') #use the name of your colab key"
      ],
      "metadata": {
        "id": "y3urI6ZIOqCf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "shap = pd.read_csv(\"shap_noincome.csv\")\n",
        "data = pd.read_csv(\"basedata.csv\")"
      ],
      "metadata": {
        "id": "ENMu9GOAO4c2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "_unAK9uuPT1U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_experimental.tools import PythonAstREPLTool\n",
        "\n",
        "tool = PythonAstREPLTool(locals={\"data\": data, \"shap\": shap})\n",
        "tool.invoke(\"data['age'].mean()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYFK3Ux7Pz-_",
        "outputId": "4a1de160-a87e-4584-ca01-5c6ab92f7dd5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38.54927397361244"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools = llm.bind_tools([tool], tool_choice=tool.name)\n",
        "llm_with_tools.invoke(\n",
        "    \"I have a dataframe 'data' and want to know the correlation between the 'age' and 'capital-gain' columns\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB_HkbpIQeTL",
        "outputId": "a5a6d3d5-d218-4262-e348-b6c00795105d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_QRNErgxKSVQ5c24W9hJSHBMO', 'function': {'arguments': '{\"query\":\"data[[\\'age\\', \\'capital-gain\\']].corr()\"}', 'name': 'python_repl_ast'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 126, 'total_tokens': 141}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-0ba71309-96d2-411f-8f1e-f4f9e7460fc6-0', tool_calls=[{'name': 'python_repl_ast', 'args': {'query': \"data[['age', 'capital-gain']].corr()\"}, 'id': 'call_QRNErgxKSVQ5c24W9hJSHBMO'}])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import JsonOutputToolsParser"
      ],
      "metadata": {
        "id": "sQWutQRMQmn8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = JsonOutputToolsParser(tool = tool)"
      ],
      "metadata": {
        "id": "dKUA_eLTRJUd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system = f\"\"\"You have access to a pandas dataframe 'data'. This dataframe contains  \\\n",
        "Here is the output of `data.head().to_markdown()`:\n",
        "\n",
        "```\n",
        "{data.head().to_markdown()}\n",
        "```\n",
        "You are also given the pandas dateframe 'shap'. Containing shap values for the test set obsevations. \\\n",
        "The shap values shows how each feature impacts the y-variable income on a local prediction level.\n",
        "```\n",
        "{shap.head().to_markdown()}\n",
        "```\n",
        "Given a user question, write the Python code to answer it. \\\n",
        "Return ONLY the valid Python code and nothing else. \\\n",
        "Don't assume you have access to any libraries other than built-in Python ones and pandas.\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{question}\")])"
      ],
      "metadata": {
        "id": "RfrF7IqQSmwj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm_with_tools | parser  # noqa\n",
        "chain.invoke({\"question\": \"What is the most important feature value for the first obsevation?'\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbG82pucTYUQ",
        "outputId": "42b27cc9-0ce4-4c8d-c1e9-aa33b5cbf3e4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'args': {'query': 'shap.iloc[0].idxmax()'}, 'type': 'python_repl_ast'}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['age'].corr(data['capital-gain'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKkxJyN7TicB",
        "outputId": "97e833f7-321b-43d9-fbc4-5ede4097a971"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08506786648520717"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "system = f\"\"\"You have access to a pandas dataframe 'data'. This is the dataframe containing actual observations and values of the dataset \\\n",
        "Here is the output of 'data.head().to_markdown()':\n",
        "\n",
        "```\n",
        "{data.head().to_markdown()}\n",
        "```\n",
        "You are also given the pandas dateframe 'shap'. Containing shap values for the test set obsevations. \\\n",
        "The shap values shows how each feature impacts the y-variable income on a local prediction level. The actual Y-variable is not available in this dataset. \\\n",
        "When asked to explain something, use the shap values for the observations to substantiate your answer.\n",
        "```\n",
        "\n",
        "{shap.head().to_markdown()}\n",
        "```\n",
        "\n",
        "The dataset contains 13K observations of descriptive data on individuals, with the prediction task being to determine whether a person makes over $50K a year.\n",
        "\n",
        "Given a user question, write the Python code to answer it. \\\n",
        "Don't assume you have access to any libraries other than built-in Python ones and pandas. \\\n",
        "Follow these steps: \\\n",
        "  1. Generete the python code you want to use. \\\n",
        "  2. Execute The Python Code \\\n",
        "  3. Answer the Question based on the exececuted Python Code in Step 2 \\\n",
        "  4. Return both the answer and the Python Code you execeuted marked in [].\"\"\"\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            system,\n",
        "        ),\n",
        "        (\"human\", \"{question}\"),\n",
        "        # This MessagesPlaceholder allows us to optionally append an arbitrary number of messages\n",
        "        # at the end of the prompt using the 'chat_history' arg.\n",
        "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def _get_chat_history(x: dict) -> list:\n",
        "    \"\"\"Parse the chain output up to this point into a list of chat history messages to insert in the prompt.\"\"\"\n",
        "    ai_msg = x[\"ai_msg\"]\n",
        "    tool_call_id = x[\"ai_msg\"].additional_kwargs[\"tool_calls\"][0][\"id\"]\n",
        "    tool_msg = ToolMessage(tool_call_id=tool_call_id, content=str(x[\"tool_output\"]))\n",
        "    return [ai_msg, tool_msg]\n",
        "\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(ai_msg=prompt | llm_with_tools)\n",
        "    .assign(tool_output=itemgetter(\"ai_msg\") | parser )\n",
        "    .assign(chat_history=_get_chat_history)\n",
        "    .assign(response=prompt | llm | StrOutputParser())\n",
        "    .pick([\"response\"])\n",
        ")"
      ],
      "metadata": {
        "id": "rebubXIabX5H"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "# This is do give the bot memory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
      ],
      "metadata": {
        "id": "YK3JR5f_Gmu6"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_chain(user_question):\n",
        "    answer = chain.invoke({\"question\": user_question})\n",
        "    return answer['response']"
      ],
      "metadata": {
        "id": "zOcIHeLTHVYF"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interface - Gradio"
      ],
      "metadata": {
        "id": "VdFC9Qni6zRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "id": "m8c6fCJIWID0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "yQQheL2hygt5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(\n",
        "    fn=call_chain,\n",
        "    inputs=[\"text\"],\n",
        "    outputs=[\"text\"]\n",
        ")"
      ],
      "metadata": {
        "id": "P4yrMdBksP1o"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "id": "nQJO8qhDsXwn",
        "outputId": "35256a28-054d-45f4-9f94-e961b1a81414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://8979562743dcba7f48.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8979562743dcba7f48.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shap.loc[0, ['marital-status_Married-civ-spouse', 'education-num', 'occupation_Other-service', 'sex_Male']]"
      ],
      "metadata": {
        "id": "U6dxhOe8_e34",
        "outputId": "220752cd-1bd3-485d-e27f-c978346214d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "marital-status_Married-civ-spouse    0.037621\n",
              "education-num                        0.053867\n",
              "occupation_Other-service             0.000020\n",
              "sex_Male                             0.004897\n",
              "Name: 0, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shap.loc[0].nlargest(5)"
      ],
      "metadata": {
        "id": "7druo21ZASDo",
        "outputId": "b3e0def5-681f-4d6a-95f2-d4a5f0878342",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "education-num                        0.102855\n",
              "marital-status_Married-civ-spouse    0.081040\n",
              "education_Bachelors                  0.052379\n",
              "relationship_Not-in-family           0.028823\n",
              "marital-status_Never-married         0.027377\n",
              "Name: 0, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "important_features = shap.iloc[0].sort_values(ascending=False)\n",
        "print(important_features)"
      ],
      "metadata": {
        "id": "06_bEAB-AWqo",
        "outputId": "6d293ee3-9562-438e-df66-b8f01d43b8bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Income                               0.247534\n",
            "education-num                        0.053867\n",
            "marital-status_Married-civ-spouse    0.037621\n",
            "education_Bachelors                  0.016926\n",
            "relationship_Not-in-family           0.011256\n",
            "                                       ...   \n",
            "occupation_Prof-specialty           -0.001490\n",
            "capital-loss                        -0.001522\n",
            "occupation_Exec-managerial          -0.003488\n",
            "hours-per-week                      -0.004886\n",
            "capital-gain                        -0.007084\n",
            "Name: 0, Length: 97, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shap['marital-status_Married-civ-spouse'].mean()"
      ],
      "metadata": {
        "id": "QlM5o_1NDasi",
        "outputId": "1c8c150a-b7e2-49a0-f5be-25eb5196b51d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0008067107222777425"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    }
  ]
}