{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing packages for operations. Pandas for storing data, random for random split, numpy for doing operations in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import random \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working direction for saving files for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the ucimlrepo for getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ucimlrepo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Adult Census Income dataset from the ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the ADULT dateset from UCI repo \n",
    "#Avaliable at https://archive.ics.uci.edu/dataset/2/adult\n",
    "import ucimlrepo\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = adult.data.features # Importing all features \n",
    "y = adult.data.targets # Importing target varialbes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recoding the y variable to match. Specifically removing the '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['income'] = y['income'].replace({'<=50K.': '<=50K', '>50K.': '>50K'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking only one instance of each outcome of the y varaible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating both x and y together in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the datatypes for any unexpected data formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the data types of all the columns\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recoding the y variable to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting the Predictor Variable into Numeric \n",
    "data['income']=data['income'].map({'<=50K':0,'>50K':1})\n",
    "data['income'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the number ? in the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of x in each column\n",
    "for col in data:\n",
    "    count = (data[col] == '?').sum()\n",
    "    print(f\"Number of values equal to {'?'} in column {col}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recoding ? to na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data == '?'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking number of na in each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping na's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the distribution of the y varialbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting the distribution of the 'income' variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='income', data=data, palette='viridis')\n",
    "plt.title('Distribution of Income')\n",
    "plt.xlabel('Income (0: <=50K, 1: >50K)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the correlation matrix for the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical features\n",
    "numerical_features = data.select_dtypes(include=['number'])\n",
    "\n",
    "# Compute the correlation matrix for numerical features only\n",
    "correlation_matrix = numerical_features.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Matrix for Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the distribution of all categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = data.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Plotting the distribution of each categorical feature\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x=feature, data=data, palette='viridis')\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the education with the education numerical column, to see their if they are redundant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    index=data[\"education\"], columns=data[\"education-num\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the procentage of the majority class in the native-country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of 'United States' in 'native country' column\n",
    "total_count = len(data)\n",
    "us_count = data['native-country'].value_counts().get('United-States', 0)\n",
    "percentage_us = (us_count / total_count) * 100\n",
    "percentage_us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing outlier classes in selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['workclass'] != 'Without-pay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['education'] != 'Preschool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['occupation'] != 'Armed-Forces']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recoding material status and removing other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_marital_status(status):\n",
    "    if status in ['Never-married', 'Married-civ-spouse']:\n",
    "        return status\n",
    "    elif status in ['Divorced', 'Separated']:\n",
    "        return 'Divorced or Separated'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to the 'marital-status' column\n",
    "data['marital-status'] = data['marital-status'].map(map_marital_status)\n",
    "\n",
    "# Drop rows with None values in 'marital-status' column\n",
    "df = data.dropna(subset=['marital-status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns based on redundancy and near zero variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['education-num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['native-country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the new distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = data.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Plotting the distribution of each categorical feature\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x=feature, data=data, palette='viridis')\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, drop_first = False, dummy_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data['income'].values\n",
    "features = [col for col in data.columns if col not in ['income']]\n",
    "X = data[features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the Random Forrest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'max_features': ['sqrt'], #['sqrt'],\n",
    "    'min_samples_leaf': [10, 15, 20, 25, 30],\n",
    "    'min_samples_split': [5, 10, 15, 20],\n",
    "    'n_estimators': [10, 15, 20, 30, 40, 50]\n",
    "    }\n",
    "\n",
    "# Initialize the RandomForestClassifier (you can choose any other classifier)\n",
    "classifier = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "n_estimators = 100\n",
    "grid_search_rf = GridSearchCV(estimator=classifier, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params_rf)\n",
    "\n",
    "\n",
    "# Use the best model for predictions\n",
    "best_model_rf = grid_search_rf.best_estimator_\n",
    "y_pred_rf = best_model_rf.predict(X_test)\n",
    "y_pred_rf_df = pd.DataFrame(best_model_rf.predict(X_test), columns=['Predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the accurary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print accuracy and classification report\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate feature importances\n",
    "#result_rf = permutation_importance(best_model_rf, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "# Get sorted indices of feature importances\n",
    "#sorted_idx_rf = result_rf.importances_mean.argsort()[-10:]  # Selecting the top 10 most important features\n",
    "\n",
    "# Plot\n",
    "#fig, ax = plt.subplots()\n",
    "#ax.boxplot(result_rf.importances[sorted_idx_rf].T, vert=False, labels=np.array(sorted_idx_rf)+1)\n",
    "#ax.set_title(\"Top 10 Variable Importance in Projection (Random Forest)\")\n",
    "#ax.set_ylabel(\"Features\")\n",
    "#fig.tight_layout()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLotting feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the best model\n",
    "feature_importances = best_model_rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame with feature names and their importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select the top 10 features\n",
    "top_10_features = feature_importance_df.head(25)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=top_10_features, palette='viridis')\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Get feature importances from the best model\n",
    "permutation_feature_importances = permutation_importance(best_model_rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "\n",
    "# Create a DataFrame with feature names and their importances\n",
    "permutation_importances = pd.Series(permutation_feature_importances.importances_mean, index=X_train.columns)\n",
    "\n",
    "# Select the top 10 highest feature importances\n",
    "top_10_importances = permutation_importances.nlargest(25)\n",
    "\n",
    "# Plot using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_10_importances.values, y=top_10_importances.index, palette='viridis')\n",
    "plt.title(\"Top 10 Feature Importances (Random Forest)\")\n",
    "plt.xlabel(\"Mean Decrease in Impurity\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the proberbilities of the first obsevation for user test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first observation from the test set\n",
    "first_observation = X_test.iloc[[0]]\n",
    "\n",
    "# Predict the class label for the first observation\n",
    "prediction = best_model_rf.predict(first_observation)\n",
    "\n",
    "# Get the predicted probabilities for each class for the first observation\n",
    "probabilities = best_model_rf.predict_proba(first_observation)\n",
    "\n",
    "# Print the prediction and probabilities\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Probabilities:\")\n",
    "for class_index, class_probability in enumerate(probabilities[0]):\n",
    "    print(f\"Class {best_model_rf.classes_[class_index]}: {class_probability:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a copy of the base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedata = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attaching the actual class, prediction and proberbility of the predicted class the the basedata dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedata['Actual Class'] = y_test\n",
    "basedata['Prediction'] = y_pred_rf\n",
    "\n",
    "\n",
    "# Add a column for the probability of the predicted class\n",
    "y_pred_proba_rf = best_model_rf.predict_proba(X_test)\n",
    "predicted_class_probs = [probs[class_index] for probs, class_index in zip(y_pred_proba_rf, y_pred_rf)]\n",
    "basedata['Predicted Class Probability'] = predicted_class_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "if basedata.isna().values.any():\n",
    "    print(\"There are missing values in the DataFrame.\")\n",
    "else:\n",
    "    print(\"There are no missing values in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to csv for the LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedata.to_csv('basedata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model explainability - Local and Global interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the SHAP package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the explainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Create an explainer for the best model\n",
    "explainer_rf = shap.Explainer(best_model_rf)\n",
    "\n",
    "shap_values_rf = explainer_rf(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the SHAP values to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing a dataframe of each - 0.26 is the base value for churn = 1, for all observations. Each variable then has a SHAP-value +- the BV\n",
    "shap_df = pd.DataFrame(\n",
    "    np.c_[shap_values_rf[:, :, 1].base_values, shap_values_rf[:, :, 1].values],\n",
    "    columns = [\"Income\"] + list(X_test.columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the SHAP dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the income from the dataframe for the LLM to interpret the SHAP values onlyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_df_noincome = shap_df.copy()\n",
    "shap_df_noincome.drop(columns=['Income'], inplace = True)\n",
    "\n",
    "shap_df_percentage_noincome = shap_df_percentage.copy()\n",
    "shap_df_percentage_noincome.drop(columns=['Income'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving SHAP value for the LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_df_noincome.to_csv('shap_noincome.csv', index=False)  # Set index=False to not write row numbers as the first column\n",
    "shap_df_percentage_noincome.to_csv('shap_percentage_noincome.csv', index=False)  # Set index=False to not write row numbers as the first column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check of Global Understanding of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_sum = np.abs(shap_values_rf[:, :, 1].values).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for better readability\n",
    "shap_importance = pd.DataFrame(list(zip(X.columns, shap_sum)), columns=['Feature', 'SHAP Value'])\n",
    "shap_importance = shap_importance.sort_values(by='SHAP Value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the global SHAP values\n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "shap_importance.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
